p8105\_hw5\_cac2225
================
Courtney Chan
November 7, 2018

Homework 5
==========

Problem 0
---------

A github repo and local R project was created, an Rmd file was produced, knitted, and committed to github.

Problem 1
---------

First loading all necessary packages

``` r
library(tidyverse)
```

    ## -- Attaching packages --- tidyverse 1.2.1 --

    ## v ggplot2 3.0.0     v purrr   0.2.5
    ## v tibble  1.4.2     v dplyr   0.7.6
    ## v tidyr   0.8.1     v stringr 1.3.1
    ## v readr   1.1.1     v forcats 0.3.0

    ## -- Conflicts ------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(rvest)
```

    ## Loading required package: xml2

    ## 
    ## Attaching package: 'rvest'

    ## The following object is masked from 'package:purrr':
    ## 
    ##     pluck

    ## The following object is masked from 'package:readr':
    ## 
    ##     guess_encoding

``` r
library(readr)
library(stringr)
```

Using list.files function to list all of the file names within the data folder and creating a dataframe for the file names. The map function and read\_csv function are used to map the contents of each csv file onto the file name dataframe.

``` r
data_folder = "./data"

file_names = list.files(data_folder)
```

``` r
reading_csv = function(x) {
  
  place_holder = read_csv(file.path(data_folder,x))
}
```

``` r
df_files = data_frame(file_names = file_names) %>% 
mutate(actual_files = map(file_names, reading_csv))
```

    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )

    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_integer(),
    ##   week_8 = col_double()
    ## )

    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )
    ## Parsed with column specification:
    ## cols(
    ##   week_1 = col_double(),
    ##   week_2 = col_double(),
    ##   week_3 = col_double(),
    ##   week_4 = col_double(),
    ##   week_5 = col_double(),
    ##   week_6 = col_double(),
    ##   week_7 = col_double(),
    ##   week_8 = col_double()
    ## )

The values within the lists are unnested into the dataframe. The dataframe is then tidied by gathering the values into a "week" variable and assigning corresponding observation values. Treatment and id variables are created. Week is mutated into a numeric variable. The treatment values are recoded into "control" and "experimental".

``` r
df_files = unnest(df_files) %>% 
  gather(key = week, value = obs_value, week_1:week_8) %>% 
    separate(week, into = c("remove", "week"), sep = "_") %>% 
  rename(treatment_id = file_names) %>%
  mutate(treatment_id = str_replace(treatment_id, ".csv", "")) %>% separate(treatment_id, into = c("treatment", "subj_id"), sep = "_") %>% select(subj_id, treatment, week, obs_value, -starts_with("remove")) %>% 
  mutate(week = as.numeric(week)) %>% 
  mutate(treatment = recode(treatment, "con" = "control", "exp" = "experimental"))

df_files
```

    ## # A tibble: 160 x 4
    ##    subj_id treatment  week obs_value
    ##    <chr>   <chr>     <dbl>     <dbl>
    ##  1 01      control       1      0.2 
    ##  2 02      control       1      1.13
    ##  3 03      control       1      1.77
    ##  4 04      control       1      1.04
    ##  5 05      control       1      0.47
    ##  6 06      control       1      2.37
    ##  7 07      control       1      0.03
    ##  8 08      control       1     -0.08
    ##  9 09      control       1      0.08
    ## 10 10      control       1      2.14
    ## # ... with 150 more rows

``` r
ggplot(df_files, aes(x = week, y = obs_value, color = subj_id)) + 
  facet_grid(.~treatment) + 
  geom_line() + 
  labs(
    title = "Observations over weeks 1-8 for each subject",
    y = "Observation Values"
  ) + 
  theme(legend.background = element_rect(size = .5))
```

![](p8105_hw5_cac2225_files/figure-markdown_github/spaghetti%20plot-1.png)

A spaghetti plot is created, graphing the changes in observation values for each individual subject. The graphs are separated by treatment type - control and experimental. There is a huge difference between the trends seen in the observation values for the control versus experimental groups. The control groups' values remain fairly low for all 8 weeks, hovering from approximately -1 to 3. Whereas amongst the experimental group there is a large, steady increase of the observational values over the 1-8 weeks, starting at around 0 to3.5 and increasing to around 3 to 6.

Problem 2
---------

The csv file is imported using the read\_csv function and the dataset is cleaned.

``` r
homicide = read_csv("./data2/homicide-data.csv") %>% 
  janitor::clean_names()
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_integer(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

Looking at the raw data, the number of observations and number of variables are 52179, 12. The variable names include uid, reported\_date, victim\_last, victim\_first, victim\_race, victim\_age, victim\_sex, city, state, lat, lon, disposition. The variables include identifiers describing the victim (id number, first and last name, sex and race), where and when the victim was found (using reported\_date, longitude and latitude coordinates, city) and whether the case was closed or still open.

A city\_state variable is created using the functions paste and mutate.

``` r
homicide = homicide %>% 
  mutate(city_state = paste(city,",",state))
```

``` r
total_num_homicides = homicide %>% 
  group_by(city_state) %>% 
  summarize(total_num_hom = n())
```

``` r
unsolved_homicides = homicide %>% 
  filter(disposition == "Closed without arrest" | disposition == "Open/No arrest") %>% 
  group_by(city_state) %>% 
  summarize(num_unsolved_hom = n())
```

The number of total homicides are added up per city\_state. The number of unsolved homicides based on disposition status is added up per city\_state. Comparing these two datsets, total\_num\_homicides and unsolved\_homicides, there are 51 and 50 observations respectively. The extra city\_state in the total\_num\_homicides set is Tusla, AL with one homicide. As a result it is removed since it is an outlier.

``` r
total_num_homicides$city_state[!(total_num_homicides$city_state %in% unsolved_homicides$city_state)]
```

    ## [1] "Tulsa , AL"

To determine the proportion of unsolved homicides over the total number homicides in Baltimore, MD, the datasets are merged by city\_state. The dataset is filtered by Baltimore, MD and the prop.test function is used to determine the proportion estimate and confidence intervals. The values are then output.

``` r
df_combined = merge(total_num_homicides, unsolved_homicides, by.x = "city_state", by.y = "city_state")

df_combined_BMD = df_combined %>% 
  filter(city_state == "Baltimore , MD")

prop_BMD = prop.test(df_combined_BMD$num_unsolved_hom, df_combined_BMD$total_num_hom)

broom::tidy(prop_BMD)
```

    ## # A tibble: 1 x 8
    ##   estimate statistic  p.value parameter conf.low conf.high method
    ##      <dbl>     <dbl>    <dbl>     <int>    <dbl>     <dbl> <chr> 
    ## 1    0.646      239. 6.46e-54         1    0.628     0.663 1-sam~
    ## # ... with 1 more variable: alternative <chr>

``` r
ouput_bmd = prop_BMD %>%
  broom::tidy() %>% 
  select(estimate, conf.low, conf.high)
```

A function is built using prop.test for the unsolved and total parameters to be input for each city\_state. The estimates and confidence interval values are selected.

``` r
prop_func <- function(unsolved, total) {
  
  prop.test(unsolved, total) %>% 
    broom::tidy() %>% 
    select(estimate, conf.low, conf.high)
}
```

``` r
df_prop = 
  df_combined %>% 
  mutate(prop_unsolved = map2(num_unsolved_hom, total_num_hom, prop_func)) %>% 
  unnest()

df_prop
```

    ##             city_state total_num_hom num_unsolved_hom  estimate  conf.low
    ## 1     Albuquerque , NM           378              146 0.3862434 0.3372604
    ## 2         Atlanta , GA           973              373 0.3833505 0.3528119
    ## 3       Baltimore , MD          2827             1825 0.6455607 0.6275625
    ## 4     Baton Rouge , LA           424              196 0.4622642 0.4141987
    ## 5      Birmingham , AL           800              347 0.4337500 0.3991889
    ## 6          Boston , MA           614              310 0.5048860 0.4646219
    ## 7         Buffalo , NY           521              319 0.6122841 0.5687990
    ## 8       Charlotte , NC           687              206 0.2998544 0.2660820
    ## 9         Chicago , IL          5535             4073 0.7358627 0.7239959
    ## 10     Cincinnati , OH           694              309 0.4452450 0.4079606
    ## 11       Columbus , OH          1084              575 0.5304428 0.5002167
    ## 12         Dallas , TX          1567              754 0.4811742 0.4561942
    ## 13         Denver , CO           312              169 0.5416667 0.4846098
    ## 14        Detroit , MI          2519             1482 0.5883287 0.5687903
    ## 15         Durham , NC           276              101 0.3659420 0.3095874
    ## 16     Fort Worth , TX           549              255 0.4644809 0.4222542
    ## 17         Fresno , CA           487              169 0.3470226 0.3051013
    ## 18        Houston , TX          2942             1493 0.5074779 0.4892447
    ## 19   Indianapolis , IN          1322              594 0.4493192 0.4223156
    ## 20   Jacksonville , FL          1168              597 0.5111301 0.4820460
    ## 21    Kansas City , MO          1190              486 0.4084034 0.3803996
    ## 22      Las Vegas , NV          1381              572 0.4141926 0.3881284
    ## 23     Long Beach , CA           378              156 0.4126984 0.3629026
    ## 24    Los Angeles , CA          2257             1106 0.4900310 0.4692208
    ## 25     Louisville , KY           576              261 0.4531250 0.4120609
    ## 26        Memphis , TN          1514              483 0.3190225 0.2957047
    ## 27          Miami , FL           744              450 0.6048387 0.5685783
    ## 28      Milwaukee , wI          1115              403 0.3614350 0.3333172
    ## 29    Minneapolis , MN           366              187 0.5109290 0.4585150
    ## 30      Nashville , TN           767              278 0.3624511 0.3285592
    ## 31    New Orleans , LA          1434              930 0.6485356 0.6231048
    ## 32       New York , NY           627              243 0.3875598 0.3494421
    ## 33        Oakland , CA           947              508 0.5364308 0.5040588
    ## 34  Oklahoma City , OK           672              326 0.4851190 0.4467861
    ## 35          Omaha , NE           409              169 0.4132029 0.3653146
    ## 36   Philadelphia , PA          3037             1360 0.4478103 0.4300380
    ## 37        Phoenix , AZ           914              504 0.5514223 0.5184825
    ## 38     Pittsburgh , PA           631              337 0.5340729 0.4942706
    ## 39       Richmond , VA           429              113 0.2634033 0.2228571
    ## 40     Sacramento , CA           376              139 0.3696809 0.3211559
    ## 41    San Antonio , TX           833              357 0.4285714 0.3947772
    ## 42 San Bernardino , CA           275              170 0.6181818 0.5576628
    ## 43      San Diego , CA           461              175 0.3796095 0.3354259
    ## 44  San Francisco , CA           663              336 0.5067873 0.4680516
    ## 45       Savannah , GA           246              115 0.4674797 0.4041252
    ## 46      St. Louis , MO          1677              905 0.5396541 0.5154369
    ## 47       Stockton , CA           444              266 0.5990991 0.5517145
    ## 48          Tampa , FL           208               95 0.4567308 0.3881009
    ## 49          Tulsa , OK           583              193 0.3310463 0.2932349
    ## 50     Washington , DC          1345              589 0.4379182 0.4112495
    ##    conf.high
    ## 1  0.4375766
    ## 2  0.4148219
    ## 3  0.6631599
    ## 4  0.5110240
    ## 5  0.4689557
    ## 6  0.5450881
    ## 7  0.6540879
    ## 8  0.3358999
    ## 9  0.7473998
    ## 10 0.4831439
    ## 11 0.5604506
    ## 12 0.5062475
    ## 13 0.5976807
    ## 14 0.6075953
    ## 15 0.4260936
    ## 16 0.5072119
    ## 17 0.3913963
    ## 18 0.5256914
    ## 19 0.4766207
    ## 20 0.5401402
    ## 21 0.4370054
    ## 22 0.4407395
    ## 23 0.4642973
    ## 24 0.5108754
    ## 25 0.4948235
    ## 26 0.3432691
    ## 27 0.6400015
    ## 28 0.3905194
    ## 29 0.5631099
    ## 30 0.3977401
    ## 31 0.6731615
    ## 32 0.4270755
    ## 33 0.5685037
    ## 34 0.5236245
    ## 35 0.4627477
    ## 36 0.4657157
    ## 37 0.5839244
    ## 38 0.5734545
    ## 39 0.3082658
    ## 40 0.4209131
    ## 41 0.4630331
    ## 42 0.6753422
    ## 43 0.4258315
    ## 44 0.5454433
    ## 45 0.5318665
    ## 46 0.5636879
    ## 47 0.6447418
    ## 48 0.5269851
    ## 49 0.3711192
    ## 50 0.4649455

Using a pipeline and the created function, a tidy dataframe with each city\_state, proportion esimate and confidence intervals is created.

``` r
df_prop %>% 
  ggplot(aes(fct_reorder(city_state, desc(estimate)), estimate)) + geom_bar(stat = "identity") + geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + theme(axis.text.x = element_text(angle = 90)) + ggtitle("estimates and confidence intervals per city, state") + xlab("city, state") + ylab("estimate of proportion of unsolved homicides")
```

![](p8105_hw5_cac2225_files/figure-markdown_github/plot%20estimates%20and%20CIs%20for%20each%20city-1.png)

A plot is created using the final dataframe, plotting city/state versus estimates of proportions of unsolved homicides/ total homicides. Confidence interval values are indicated. The city/states are organized from largest proportion of unsolved homicides to smallest proportion. Chicago IL has the highest proportion of unsolved homicides while Richmond, Va has the lowest proportion of unsolved homicides.
